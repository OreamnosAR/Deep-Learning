{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OreamnosAR/Deep-Learning/blob/main/w8_gpu_debugging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While using the GPU has some very tangible benefits in terms of execution speed of our code, combining CPU and GPU also comes with some further complications. In other words, new and exciting error messages! üòè\n",
        "\n",
        "In this notebook, you will be presented with various simple training setups, which are all failing to run for different reasons. The task in each case is to determine what the cause of the failure is, and fix it.\n",
        "\n",
        "First we need to make sure that you remembered to turn on the GPU:"
      ],
      "metadata": {
        "id": "quAQavmIqevD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print('Congrats, you have a gpu :)')\n",
        "else:\n",
        "  print('Woups, please go go into edit -> notebook settings and give yourself some hardware acceleration')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlSSxSMsqeMR",
        "outputId": "3a342cfc-8b4b-4fdb-bff8-32277e4612d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Congrats, you have a gpu :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our example today, Alice has a small, working bit of pytorch code.\n",
        "However, yesterday was Alice's birthday, and Bob gave her a shiny new GPU. So, today, Alice wants to upgrade her code to take advantage of her new hardware.\n",
        "\n",
        "This is what Alice's code looked like before she starting upgrading:"
      ],
      "metadata": {
        "id": "5WNnGRFQs9cj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Sk2fheSrqbS2"
      },
      "outputs": [],
      "source": [
        "#gpu-free code:\n",
        "import torch.nn as nn\n",
        "\n",
        "Xset=torch.utils.data.TensorDataset(torch.rand(1000000,100),torch.rand(1000000,1))\n",
        "trainLoader=torch.utils.data.DataLoader(Xset,batch_size=32)\n",
        "\n",
        "model=nn.Sequential(nn.Linear(100,5),nn.ReLU(),\n",
        "                    nn.Linear(5,1))\n",
        "\n",
        "lossF=nn.functional.huber_loss\n",
        "optimizer=torch.optim.Adam(model.parameters())\n",
        "\n",
        "for xbatch,ybatch in trainLoader:\n",
        "  pred=model(xbatch)\n",
        "  loss=lossF(ybatch,pred)\n",
        "\n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This runs fine.\n",
        "Now alice makes a gpu-device, and moves the calculations there:"
      ],
      "metadata": {
        "id": "XKx29JybzmSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda')\n",
        "\n",
        "model.to(device)\n",
        "optimizer=torch.optim.Adam(model.parameters())\n",
        "\n",
        "nEpochs=10\n",
        "for xbatch,ybatch in trainLoader:\n",
        "  xbatch = xbatch.to(device)\n",
        "  ybatch = ybatch.to(device)\n",
        "  pred=model(xbatch)\n",
        "  loss=lossF(ybatch,pred)\n",
        "\n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "la4P0NLmzlvY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1:\n",
        "Get the code above to run, on the GPU.\n",
        "\n",
        "Having solved the first hurdle, Alice decides to further upgrade the model with an extra offset:"
      ],
      "metadata": {
        "id": "V_Vcsj9i0wTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class aliceModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer1=nn.Linear(100,5)\n",
        "    self.layer2=nn.Linear(5,1)\n",
        "\n",
        "    self.extraOffset=torch.rand((1,5),requires_grad=True)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    #modify the code to explicitly move self.extraOffset to the device within the forward method:\n",
        "    extra_offset = self.extraOffset.to(x.device)\n",
        "    x=self.layer1(x)+extra_offset\n",
        "    #x=self.layer1(x)+self.extraOffset\n",
        "    x=nn.functional.relu(x)\n",
        "    x=self.layer2(x)\n",
        "    x=nn.functional.relu(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "#test:\n",
        "net=aliceModel()\n",
        "net(torch.rand((10,100)));"
      ],
      "metadata": {
        "id": "0QD2dd28C2-c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, when she now runs her training loop:"
      ],
      "metadata": {
        "id": "fRr-mPEyF5cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda')\n",
        "model=aliceModel()\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer=torch.optim.Adam(model.parameters())\n",
        "\n",
        "for xbatch,ybatch in trainLoader:\n",
        "  xbatch=xbatch.to(device)\n",
        "  ybatch=ybatch.to(device)\n",
        "\n",
        "  pred=model(xbatch)\n",
        "  loss=lossF(ybatch,pred)\n",
        "\n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "mVPJ9seKF5EH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2:\n",
        "Solve this new GPU issue\n",
        "\n",
        "Feeling now quite an expert on pytorch and devices, Alice wishes to speed up her training by increasing the batch size. To make sure she doesn't run out of memory, she checks the free memory on the GPU first, and sets the batch accordingly:"
      ],
      "metadata": {
        "id": "jXfwOFgIGQaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class aliceModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer1=nn.Linear(100,5)\n",
        "    self.layer2=nn.Linear(5,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.layer1(x)\n",
        "    x=nn.functional.relu(x)\n",
        "    x=self.layer2(x)\n",
        "    x=nn.functional.relu(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "#test:\n",
        "device=torch.device('cuda')\n",
        "net=aliceModel()\n",
        "net(torch.rand((10,100)));\n",
        "\n",
        "\n",
        "from math import floor\n",
        "\n",
        "t = torch.cuda.get_device_properties(device).total_memory\n",
        "print('total memory: ',t/1e6,'mb')\n",
        "\n",
        "batchSize=floor(t/Xset[0][0].element_size()/2) #this is less than half of the space on the GPU\n",
        "print('batch size: ',batchSize)\n",
        "\n"
      ],
      "metadata": {
        "id": "3q_pcUpCZtnj",
        "outputId": "c4440b55-62c3-432d-d2ef-c95da040394f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total memory:  15835.660288 mb\n",
            "batch size:  1979457536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der er en fejl i koden over... batchsize udregnes udfra et element i datas√¶ttet, og ikke hele datas√¶ttet st√∏rrelse... Er rettet i koden neden under:"
      ],
      "metadata": {
        "id": "T0zOCJzEFv8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xset[0][0].element_size()\n",
        "t/4"
      ],
      "metadata": {
        "id": "I4cZ9PNKBeTY",
        "outputId": "55e6343d-c63c-4d01-cb94-8e73de32cd8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3958915072.0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(Xset)"
      ],
      "metadata": {
        "id": "2hLve1HKCUYk",
        "outputId": "1f8ae100-8881-4fae-88a2-5ec37483fdf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batchSize=floor(t/(Xset[0][0].element_size()*len(Xset)))\n",
        "print('batch size: ',batchSize)"
      ],
      "metadata": {
        "id": "XnPn8xq7C8Q7",
        "outputId": "53df5289-db18-416e-a1c4-a773bcdbecdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch size:  3958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainLoader=torch.utils.data.DataLoader(Xset,batch_size=batchSize)\n",
        "\n",
        "model=aliceModel()\n",
        "model.to(device)\n",
        "optimizer=torch.optim.Adam(model.parameters())\n",
        "\n",
        "for xbatch,ybatch in trainLoader:\n",
        "  xbatch=xbatch.to(device)\n",
        "  ybatch=ybatch.to(device)\n",
        "\n",
        "  pred=model(xbatch)\n",
        "  loss=lossF(ybatch,pred)\n",
        "\n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n"
      ],
      "metadata": {
        "id": "DAVgG53vU6Pg"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}